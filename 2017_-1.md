- [SLAC: A Sparsely Labeled Dataset for Action Classification and  Localization](http://arxiv.org/abs/1712.09374v1), Hang Zhao, Zhicheng Yan, Heng Wang, Lorenzo Torresani, Antonio Torralba

This paper describes a procedure for the creation of large-scale video
datasets for action classification and localization from unconstrained,
realistic web data. The scalability of the proposed procedure is demonstrated
by building a novel video benchmark, named SLAC (Sparsely Labeled ACtions),
consisting of over 520K untrimmed videos and 1.75M clip annotations spanning
200 action categories. Using our proposed framework, annotating a clip takes
merely 8.8 seconds on average. This represents a saving in labeling time of
over 95% compared to the traditional procedure of manual trimming and
localization of actions. Our approach dramatically reduces the amount of human
labeling by automatically identifying hard clips, i.e., clips that contain
coherent actions but lead to prediction disagreement between action
classifiers. A human annotator can disambiguate whether such a clip truly
contains the hypothesized action in a handful of seconds, thus generating
labels for highly informative samples at little cost. We show that our
large-scale dataset can be used to effectively pre-train action recognition
models, significantly improving final metrics on smaller-scale benchmarks after
fine-tuning. On Kinetics, UCF-101 and HMDB-51, models pre-trained on SLAC
outperform baselines trained from scratch, by 2.0%, 20.1% and 35.4% in top-1
accuracy, respectively when RGB input is used. Furthermore, we introduce a
simple procedure that leverages the sparse labels in SLAC to pre-train action
localization models. On THUMOS14 and ActivityNet-v1.3, our localization model
improves the mAP of baseline model by 8.6% and 2.5%, respectively.

- [Smart, Sparse Contours to Represent and Edit Images](http://arxiv.org/abs/1712.08232v1), Tali Dekel, Chuang Gan, Dilip Krishnan, Ce Liu, William T. Freeman

We study the problem of reconstructing an image from information stored at
sparse contour locations. Existing contour-based image reconstruction methods
struggle to balance contour sparsity and reconstruction fidelity. Therefore,
denser contours are needed to capture subtle texture information even though
contours were not meant for textures. We propose a novel image representation
where image content is characterized by contours with gradient information via
an encoder-decoder network, while image details are modeled by a conditional
generative adversarial network. We show that high-quality reconstructions with
high fidelity to the source image can be obtained from extremely sparse input,
e.g., comprising less than 6% of image pixels. Our model synthesizes texture,
details and fine structures in regions where no input information is provided.
The semantic knowledge encoded into our model and the sparsity of the input
allows using contours as an intuitive interface for semantically-aware image
manipulation: local edits in contour domain such as scaling, translation and
erasing, translate to long-range and coherent changes in the pixel space.
Experiments on a variety of datasets verify the versatility and convenience
afforded by our models.

- [Consensus-based Sequence Training for Video Captioning](http://arxiv.org/abs/1712.09532v1), Sang Phan, Gustav Eje Henter, Yusuke Miyao, Shin'ichi Satoh

Captioning models are typically trained using the cross-entropy loss.
However, their performance is evaluated on other metrics designed to better
correlate with human assessments. Recently, it has been shown that
reinforcement learning (RL) can directly optimize these metrics in tasks such
as captioning. However, this is computationally costly and requires specifying
a baseline reward at each step to make training converge. We propose a fast
approach to optimize one's objective of interest through the REINFORCE
algorithm. First we show that, by replacing model samples with ground-truth
sentences, RL training can be seen as a form of weighted cross-entropy loss,
giving a fast, RL-based pre-training algorithm. Second, we propose to use the
consensus among ground-truth captions of the same video as the baseline reward.
This can be computed very efficiently. We call the complete proposal
Consensus-based Sequence Training (CST). Applied to the MSRVTT video captioning
benchmark, our proposals train significantly faster than comparable methods and
establish a new state-of-the-art on the task, improving the CIDEr score from
47.3 to 54.2.

- [Exploring the Space of Black-box Attacks on Deep Neural Networks](http://arxiv.org/abs/1712.09491v1), Arjun Nitin Bhagoji, Warren He, Bo Li, Dawn Song

Existing black-box attacks on deep neural networks (DNNs) so far have largely
focused on transferability, where an adversarial instance generated for a
locally trained model can "transfer" to attack other learning models. In this
paper, we propose novel Gradient Estimation black-box attacks for adversaries
with query access to the target model's class probabilities, which do not rely
on transferability. We also propose strategies to decouple the number of
queries required to generate each adversarial sample from the dimensionality of
the input. An iterative variant of our attack achieves close to 100%
adversarial success rates for both targeted and untargeted attacks on DNNs. We
carry out extensive experiments for a thorough comparative evaluation of
black-box attacks and show that the proposed Gradient Estimation attacks
outperform all transferability based black-box attacks we tested on both MNIST
and CIFAR-10 datasets, achieving adversarial success rates similar to well
known, state-of-the-art white-box attacks. We also apply the Gradient
Estimation attacks successfully against a real-world Content Moderation
classifier hosted by Clarifai. Furthermore, we evaluate black-box attacks
against state-of-the-art defenses. We show that the Gradient Estimation attacks
are very effective even against these defenses.

- [Detect-and-Track: Efficient Pose Estimation in Videos](http://arxiv.org/abs/1712.09184v1), Rohit Girdhar, Georgia Gkioxari, Lorenzo Torresani, Manohar Paluri, Du Tran

This paper addresses the problem of estimating and tracking human body
keypoints in complex, multi-person video. We propose an extremely lightweight
yet highly effective approach that builds upon the latest advancements in human
detection and video understanding. Our method operates in two-stages: keypoint
estimation in frames or short clips, followed by lightweight tracking to
generate keypoint predictions linked over the entire video. For frame-level
pose estimation we experiment with Mask R-CNN, as well as our own proposed 3D
extension of this model, which leverages temporal information over small clips
to generate more robust frame predictions. We conduct extensive ablative
experiments on the newly released multi-person video pose estimation benchmark,
PoseTrack, to validate various design choices of our model. Our approach
achieves an accuracy of 55.2% on the validation and 51.8% on the test set using
the Multi-Object Tracking Accuracy (MOTA) metric, and achieves state of the art
performance on the ICCV 2017 PoseTrack keypoint tracking challenge.

- [Deep Meta Learning for Real-Time Visual Tracking based on  Target-Specific Feature Space](http://arxiv.org/abs/1712.09153v1), Janghoon Choi, Junseok Kwon, Kyoung Mu Lee

In this paper, we propose a novel on-line visual tracking framework based on
Siamese matching network and meta-learner network which runs at real-time
speed. Conventional deep convolutional feature based discriminative visual
tracking algorithms require continuous re-training of classifiers or
correlation filters for solving complex optimization tasks to adapt to the new
appearance of a target object. To remove this process, our proposed algorithm
incorporates and utilizes a meta-learner network to provide the matching
network with new appearance information of the target object by adding the
target-aware feature space. The parameters for the target-specific feature
space are provided instantly from a single forward-pass of the meta-learner
network. By eliminating the necessity of continuously solving the complex
optimization tasks in the course of tracking, experimental results demonstrate
that our algorithm performs at a real-time speed of $62$ fps while maintaining
a competitive performance among other state-of-the-art tracking algorithms.

- [Interpretable Counting for Visual Question Answering](http://arxiv.org/abs/1712.08697v1), Alexander Trott, Caiming Xiong, Richard Socher

Questions that require counting a variety of objects in images remain a major
challenge in visual question answering (VQA). The most common approaches to VQA
involve either classifying answers based on fixed length representations of
both the image and question or summing fractional counts estimated from each
section of the image. In contrast, we treat counting as a sequential decision
process and force our model to make discrete choices of what to count.
Specifically, the model sequentially selects from detected objects and learns
interactions between objects that influence subsequent selections. A
distinction of our approach is its intuitive and interpretable output, as
discrete counts are automatically grounded in the image. Furthermore, our
method outperforms the state of the art architecture for VQA on multiple
metrics that evaluate counting.

- [Future Frame Prediction for Anomaly Detection -- A New Baseline](http://arxiv.org/abs/1712.09867v1), Wen Liu, Weixin Luo, Dongze Lian, Shenghua Gao

Anomaly detection in videos refers to the identification of events that do
not conform to expected behavior. However, almost all existing methods tackle
the problem by minimizing the reconstruction errors of training data, which
cannot guarantee a larger reconstruction error for an abnormal event. In this
paper, we propose to tackle the anomaly detection problem within a video
prediction framework. To the best of our knowledge, this is the first work that
leverages the difference between a predicted future frame and its ground truth
to detect an abnormal event. To predict a future frame with higher quality for
normal events, other than the commonly used appearance (spatial) constraints on
intensity and gradient, we also introduce a motion (temporal) constraint in
video prediction by enforcing the optical flow between predicted frames and
ground truth frames to be consistent, and this is the first work that
introduces a temporal constraint into the video prediction task. Such spatial
and motion constraints facilitate the future frame prediction for normal
events, and consequently facilitate to identify those abnormal events that do
not conform the expectation. Extensive experiments on both a toy dataset and
some publicly available datasets validate the effectiveness of our method in
terms of robustness to the uncertainty in normal events and the sensitivity to
abnormal events

- [Taking Visual Motion Prediction To New Heightfields](http://arxiv.org/abs/1712.09448v1), Sebastien Ehrhardt, Aron Monszpart, Niloy Mitra, Andrea Vedaldi

While the basic laws of Newtonian mechanics are well understood, explaining a
physical scenario still requires manually modeling the problem with suitable
equations and estimating the associated parameters. In order to be able to
leverage the approximation capabilities of artificial intelligence techniques
in such physics related contexts, researchers have handcrafted the relevant
states, and then used neural networks to learn the state transitions using
simulation runs as training data. Unfortunately, such approaches are unsuited
for modeling complex real-world scenarios, where manually authoring relevant
state spaces tend to be tedious and challenging. In this work, we investigate
if neural networks can implicitly learn physical states of real-world
mechanical processes only based on visual data while internally modeling
non-homogeneous environment and in the process enable long-term physical
extrapolation. We develop a recurrent neural network architecture for this task
and also characterize resultant uncertainties in the form of evolving variance
estimates. We evaluate our setup to extrapolate motion of rolling ball(s) on
bowls of varying shape and orientation, and on arbitrary heightfields using
only images as input. We report significant improvements over existing
image-based methods both in terms of accuracy of predictions and complexity of
scenarios; and report competitive performance with approaches that, unlike us,
assume access to internal physical states.

- [Learning Intelligent Dialogs for Bounding Box Annotation](http://arxiv.org/abs/1712.08087v1), Ksenia Konyushkova, Jasper Uijlings, Christoph Lampert, Vittorio Ferrari

We introduce Intelligent Annotation Dialogs for bounding box annotation. We
train an agent to automatically choose a sequence of actions for a human
annotator to produce a bounding box in a minimal amount of time. Specifically,
we consider two actions: box verification [37], where the annotator verifies a
box generated by an object detector, and manual box drawing. We explore two
kinds of agents, one based on predicting the probability that a box will be
positively verified, and the other based on reinforcement learning. We
demonstrate that (1) our agents are able to learn efficient annotation
strategies in several scenarios, automatically adapting to the difficulty of an
input image, the desired quality of the boxes, the strength of the detector,
and other factors; (2) in all scenarios the resulting annotation dialogs speed
up annotation compared to manual box drawing alone and box verification alone,
while also out- performing any fixed combination of verification and draw- ing
in most scenarios; (3) in a realistic scenario where the detector is
iteratively re-trained, our agents evolve a series of strategies that reflect
the shifting trade-off between verification and drawing as the detector grows
stronger.

- [Advances in Pre-Training Distributed Word Representations](http://arxiv.org/abs/1712.09405v1), Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, Armand Joulin

Many Natural Language Processing applications nowadays rely on pre-trained
word representations estimated from large text corpora such as news
collections, Wikipedia and Web Crawl. In this paper, we show how to train
high-quality word vector representations by using a combination of known tricks
that are however rarely used together. The main result of our work is the new
set of publicly available pre-trained models that outperform the current state
of the art by a large margin on a number of tasks.

- [Unifying Map and Landmark Based Representations for Visual Navigation](http://arxiv.org/abs/1712.08125v1), Saurabh Gupta, David Fouhey, Sergey Levine, Jitendra Malik

This works presents a formulation for visual navigation that unifies map
based spatial reasoning and path planning, with landmark based robust plan
execution in noisy environments. Our proposed formulation is learned from data
and is thus able to leverage statistical regularities of the world. This allows
it to efficiently navigate in novel environments given only a sparse set of
registered images as input for building representations for space. Our
formulation is based on three key ideas: a learned path planner that outputs
path plans to reach the goal, a feature synthesis engine that predicts features
for locations along the planned path, and a learned goal-driven closed loop
controller that can follow plans given these synthesized features. We test our
approach for goal-driven navigation in simulated real world environments and
report performance gains over competitive baseline approaches.

- [Sim2Real View Invariant Visual Servoing by Recurrent Control](http://arxiv.org/abs/1712.07642v1), Fereshteh Sadeghi, Alexander Toshev, Eric Jang, Sergey Levine

Humans are remarkably proficient at controlling their limbs and tools from a
wide range of viewpoints and angles, even in the presence of optical
distortions. In robotics, this ability is referred to as visual servoing:
moving a tool or end-point to a desired location using primarily visual
feedback. In this paper, we study how viewpoint-invariant visual servoing
skills can be learned automatically in a robotic manipulation scenario. To this
end, we train a deep recurrent controller that can automatically determine
which actions move the end-point of a robotic arm to a desired object. The
problem that must be solved by this controller is fundamentally ambiguous:
under severe variation in viewpoint, it may be impossible to determine the
actions in a single feedforward operation. Instead, our visual servoing system
must use its memory of past movements to understand how the actions affect the
robot motion from the current viewpoint, correcting mistakes and gradually
moving closer to the target. This ability is in stark contrast to most visual
servoing methods, which either assume known dynamics or require a calibration
phase. We show how we can learn this recurrent controller using simulated data
and a reinforcement learning objective. We then describe how the resulting
model can be transferred to a real-world robot by disentangling perception from
control and only adapting the visual layers. The adapted model can servo to
previously unseen objects from novel viewpoints on a real-world Kuka IIWA
robotic arm. For supplementary videos, see:
https://fsadeghi.github.io/Sim2RealViewInvariantServo

- [SuperPoint: Self-Supervised Interest Point Detection and Description](http://arxiv.org/abs/1712.07629v2), Daniel DeTone, Tomasz Malisiewicz, Andrew Rabinovich

This paper presents a self-supervised framework for training interest point
detectors and descriptors suitable for a large number of multiple-view geometry
problems in computer vision. As opposed to patch-based neural networks, our
fully-convolutional model operates on full-sized images and jointly computes
pixel-level interest point locations and associated descriptors in one forward
pass. We introduce Homographic Adaptation, a multi-scale, multi-homography
approach for boosting interest point detection accuracy and performing
cross-domain adaptation (e.g., synthetic-to-real). Our model, when trained on
the MS-COCO generic image dataset using Homographic Adaptation, is able to
repeatedly detect a much richer set of interest points than the initial
pre-adapted deep model and any other traditional corner detector. The final
system gives rise to strong interest point repeatability on the HPatches
dataset and outperforms traditional descriptors such as ORB and SIFT on point
matching accuracy and on the task of homography estimation.

- [Learning Sight from Sound: Ambient Sound Provides Supervision for Visual  Learning](http://arxiv.org/abs/1712.07271v1), Andrew Owens, Jiajun Wu, Josh H. McDermott, William T. Freeman, Antonio Torralba

The sound of crashing waves, the roar of fast-moving cars -- sound conveys
important information about the objects in our surroundings. In this work, we
show that ambient sounds can be used as a supervisory signal for learning
visual models. To demonstrate this, we train a convolutional neural network to
predict a statistical summary of the sound associated with a video frame. We
show that, through this process, the network learns a representation that
conveys information about objects and scenes. We evaluate this representation
on several recognition tasks, finding that its performance is comparable to
that of other state-of-the-art unsupervised learning methods. Finally, we show
through visualizations that the network learns units that are selective to
objects that are often associated with characteristic sounds. This paper
extends an earlier conference paper, Owens et al. 2016, with additional
experiments and discussion.

- [Learning with Imprinted Weights](http://arxiv.org/abs/1712.07136v1), Hang Qi, Matthew Brown, David G. Lowe

Human vision is able to immediately recognize novel visual categories after
seeing just one or a few training examples. We describe how to add a similar
capability to ConvNet classifiers by directly setting the final layer weights
from novel training examples during low-shot learning. We call this process
weight imprinting as it directly sets weights for a new category based on an
appropriately scaled copy of the embedding layer activations for that training
example. The imprinting process provides a valuable complement to training with
stochastic gradient descent, as it provides immediate good classification
performance and an initialization for any further fine-tuning in the future. We
show how this imprinting process is related to proxy-based embeddings. However,
it differs in that only a single imprinted weight vector is learned for each
novel category, rather than relying on a nearest-neighbor distance to training
instances as typically used with embedding methods. Our experiments show that
using averaging of imprinted weights provides better generalization than using
nearest-neighbor instance embeddings.

- [On the Integration of Optical Flow and Action Recognition](http://arxiv.org/abs/1712.08416v1), Laura Sevilla-Lara, Yiyi Liao, Fatma Guney, Varun Jampani, Andreas Geiger, Michael J. Black

Most of the top performing action recognition methods use optical flow as a
"black box" input. Here we take a deeper look at the combination of flow and
action recognition, and investigate why optical flow is helpful, what makes a
flow method good for action recognition, and how we can make it better. In
particular, we investigate the impact of different flow algorithms and input
transformations to better understand how these affect a state-of-the-art action
recognition method. Furthermore, we fine tune two neural-network flow methods
end-to-end on the most widely used action recognition dataset (UCF101). Based
on these experiments, we make the following five observations: 1) optical flow
is useful for action recognition because it is invariant to appearance, 2)
optical flow methods are optimized to minimize end-point-error (EPE), but the
EPE of current methods is not well correlated with action recognition
performance, 3) for the flow methods tested, accuracy at boundaries and at
small displacements is most correlated with action recognition performance, 4)
training optical flow to minimize classification error instead of minimizing
EPE improves recognition performance, and 5) optical flow learned for the task
of action recognition differs from traditional optical flow especially inside
the human body and at the boundary of the body. These observations may
encourage optical flow researchers to look beyond EPE as a goal and guide
action recognition researchers to seek better motion cues, leading to a tighter
integration of the optical flow and action recognition communities.

- [Learning to Act Properly: Predicting and Explaining Affordances from  Images](http://arxiv.org/abs/1712.07576v1), Ching-Yao Chuang, Jiaman Li, Antonio Torralba, Sanja Fidler

We address the problem of affordance reasoning in diverse scenes that appear
in the real world. Affordances relate the agent's actions to their effects when
taken on the surrounding objects. In our work, we take the egocentric view of
the scene, and aim to reason about action-object affordances that respect both
the physical world as well as the social norms imposed by the society. We also
aim to teach artificial agents why some actions should not be taken in certain
situations, and what would likely happen if these actions would be taken. We
collect a new dataset that builds upon ADE20k, referred to as ADE-Affordance,
which contains annotations enabling such rich visual reasoning. We propose a
model that exploits Graph Neural Networks to propagate contextual information
from the scene in order to perform detailed affordance reasoning about each
object. Our model is showcased through various ablation studies, pointing to
successes and challenges in this complex task.

- [Adversarial Patch](http://arxiv.org/abs/1712.09665v1), Tom B. Brown, Dandelion Mané, Aurko Roy, Martín Abadi, Justin Gilmer

We present a method to create universal, robust, targeted adversarial image
patches in the real world. The patches are universal because they can be used
to attack any scene, robust because they work under a wide variety of
transformations, and targeted because they can cause a classifier to output any
target class. These adversarial patches can be printed, added to any scene,
photographed, and presented to image classifiers; even when the patches are
small, they cause the classifiers to ignore the other items in the scene and
report a chosen target class.
